{
  "metadata": {
    "issue": "Database Connection Pool Exhaustion in Transaction Processing Service"
  },
  "mainContentBody": {
    "whatHappened": "During peak transaction hours, the Transaction Processing Service began rejecting incoming requests with timeout errors. End users saw “Service Unavailable” messages when attempting to complete payments. Simultaneously, monitoring alerts indicated that the database connection pool had reached its maximum size and new connections were being refused.",  :contentReference[oaicite:0]{index=0}:contentReference[oaicite:1]{index=1}
    "whyItHappened": "The default connection pool size was set to 50, which was sufficient under normal load but inadequate during surge traffic. A misconfiguration in the autoscaling rules for the service meant that additional application instances were not spun up quickly enough to share the load. As a result, all available database connections were consumed by long‐running queries and idle sessions, preventing new requests from acquiring a connection.",  :contentReference[oaicite:2]{index=2}:contentReference[oaicite:3]{index=3}
    "howResolved": [
      "Increased the connection pool maximum to 200 and reduced the idle timeout for unused connections from 30 minutes to 5 minutes.",
      "Corrected the autoscaling policy so that additional service pods are provisioned when CPU utilization exceeds 60% for more than two minutes.",
      "Verified that after deployment, the service maintained healthy connection levels even under peak load."
    ],
    "recurrenceLikelihood": "Low–moderate. The pool size and autoscaling rules now accommodate the current peak transaction volume plus a 30% buffer. However, if transaction volume grows unexpectedly or query performance degrades, we could again approach the new limits.",  :contentReference[oaicite:6]{index=6}:contentReference[oaicite:7]{index=7}
    "preventiveSteps": [
      "Implement continuous monitoring with proactive alerts when connection pool usage exceeds 70%.",
      "Run annual load‐testing exercises to validate pool size and autoscaling thresholds.",
      "Review and optimize any queries that hold connections for longer than five seconds."
    ],
    "howToFix": [
      "Tune HikariCP Settings: In application.properties (or application.yml), set spring.datasource.hikari.maximum-pool-size=200, idle-timeout=300000, max-lifetime=1800000, connection-timeout=30000.",
      "Enforce Socket Timeouts: Configure JDBC URL with socketTimeout=30 to prevent hanging validation checks (e.g., jdbc:postgresql://host:port/db?socketTimeout=30).",
      "Implement Connection Leak Detection: Enable HikariCP’s leak detection threshold (e.g., spring.datasource.hikari.leak-detection-threshold=20000).",
      "Optimize Autoscaling Rules: Ensure HPA triggers at CPU > 60% for ≥ 2 minutes and validate via load tests.",
      "Add Pool Usage Dashboarding: Expose HikariCP metrics (via Micrometer) to Grafana and alert when poolUsage > 0.7."
    ]
  }
}
